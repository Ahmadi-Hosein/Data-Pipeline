{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e425246-fc35-460b-82eb-302991b6ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from xclim.sdba import EmpiricalQuantileMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685bff8d-abf0-4e90-ac8a-f310e7187564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the specific latitude and longitude for extraction\n",
    "# Bridge 1 longitute and latitute\n",
    "lat_brg = 'Latitude\"\n",
    "lon_brg = \"Longitude\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148443d-0133-482a-be2c-659022422034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquring Data from NEX/GDDP-CMIP6 Web Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830face3-1eeb-4bee-bf52-ad69e5943245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the specific latitude and longitude for extraction\n",
    "lat_brg = 37.210130\n",
    "lon_brg = 260.445114  # Adjusted longitude to match dataset's convention (260.445114 corresponds to -99.554886)\n",
    "\n",
    "# List of base URLs\n",
    "base_urls = [\n",
    " 'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/CESM2/historical/r4i1p1f1/pr/pr_day_CESM2_historical_r4i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T00:00:00Z&time_end={year}-12-31T00:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/CESM2-WACCM/historical/r3i1p1f1/pr/pr_day_CESM2-WACCM_historical_r3i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T00:00:00Z&time_end={year}-12-31T00:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/CanESM5/historical/r1i1p1f1/pr/pr_day_CanESM5_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/GFDL-CM4/historical/r1i1p1f1/pr/pr_day_GFDL-CM4_historical_r1i1p1f1_gr1_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/GFDL-ESM4/historical/r1i1p1f1/pr/pr_day_GFDL-ESM4_historical_r1i1p1f1_gr1_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/GISS-E2-1-G/historical/r1i1p1f2/pr/pr_day_GISS-E2-1-G_historical_r1i1p1f2_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/ACCESS-CM2/historical/r1i1p1f1/pr/pr_day_ACCESS-CM2_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/ACCESS-ESM1-5/historical/r1i1p1f1/pr/pr_day_ACCESS-ESM1-5_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/BCC-CSM2-MR/historical/r1i1p1f1/pr/pr_day_BCC-CSM2-MR_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/CMCC-CM2-SR5/historical/r1i1p1f1/pr/pr_day_CMCC-CM2-SR5_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/CMCC-ESM2/historical/r1i1p1f1/pr/pr_day_CMCC-ESM2_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.5&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/CNRM-CM6-1/historical/r1i1p1f2/pr/pr_day_CNRM-CM6-1_historical_r1i1p1f2_gr_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/CNRM-ESM2-1/historical/r1i1p1f2/pr/pr_day_CNRM-ESM2-1_historical_r1i1p1f2_gr_{year}.nc?var=pr&north=37.4&west=260.&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/EC-Earth3/historical/r1i1p1f1/pr/pr_day_EC-Earth3_historical_r1i1p1f1_gr_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/EC-Earth3-Veg-LR/historical/r1i1p1f1/pr/pr_day_EC-Earth3-Veg-LR_historical_r1i1p1f1_gr_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/FGOALS-g3/historical/r3i1p1f1/pr/pr_day_FGOALS-g3_historical_r3i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/GFDL-CM4_gr2/historical/r1i1p1f1/pr/pr_day_GFDL-CM4_historical_r1i1p1f1_gr2_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/HadGEM3-GC31-LL/historical/r1i1p1f3/pr/pr_day_HadGEM3-GC31-LL_historical_r1i1p1f3_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-30T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/HadGEM3-GC31-MM/historical/r1i1p1f3/pr/pr_day_HadGEM3-GC31-MM_historical_r1i1p1f3_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-30T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/IITM-ESM/historical/r1i1p1f1/pr/pr_day_IITM-ESM_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-02T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/INM-CM4-8/historical/r1i1p1f1/pr/pr_day_INM-CM4-8_historical_r1i1p1f1_gr1_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/INM-CM5-0/historical/r1i1p1f1/pr/pr_day_INM-CM5-0_historical_r1i1p1f1_gr1_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/IPSL-CM6A-LR/historical/r1i1p1f1/pr/pr_day_IPSL-CM6A-LR_historical_r1i1p1f1_gr_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/KACE-1-0-G/historical/r1i1p1f1/pr/pr_day_KACE-1-0-G_historical_r1i1p1f1_gr_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-30T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/KIOST-ESM/historical/r1i1p1f1/pr/pr_day_KIOST-ESM_historical_r1i1p1f1_gr1_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T00:00:00Z&time_end={year}-12-31T00:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/MIROC-ES2L/historical/r1i1p1f2/pr/pr_day_MIROC-ES2L_historical_r1i1p1f2_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/MPI-ESM1-2-HR/historical/r1i1p1f1/pr/pr_day_MPI-ESM1-2-HR_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/MPI-ESM1-2-LR/historical/r1i1p1f1/pr/pr_day_MPI-ESM1-2-LR_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/MRI-ESM2-0/historical/r1i1p1f1/pr/pr_day_MRI-ESM2-0_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/NESM3/historical/r1i1p1f1/pr/pr_day_NESM3_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/NorESM2-LM/historical/r1i1p1f1/pr/pr_day_NorESM2-LM_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/NorESM2-MM/historical/r1i1p1f1/pr/pr_day_NorESM2-MM_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/TaiESM1/historical/r1i1p1f1/pr/pr_day_TaiESM1_historical_r1i1p1f1_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-31T12:00:00Z&&&accept=netcdf3'\n",
    ",'https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/UKESM1-0-LL/historical/r1i1p1f2/pr/pr_day_UKESM1-0-LL_historical_r1i1p1f2_gn_{year}.nc?var=pr&north=37.4&west=260.0&east=260.4&south=37.1&horizStride=1&time_start={year}-01-01T12:00:00Z&time_end={year}-12-30T12:00:00Z&&&accept=netcdf3'\n",
    "]\n",
    "\n",
    "\n",
    "# List of years to download and process from 1981 to 2010 (30 years)\n",
    "years = range(Start Year, End Year)\n",
    "\n",
    "for base_url in base_urls:\n",
    "    # Extract a unique part of the base_url to use as the folder name\n",
    "    folder_name = base_url.split('/')[-5]  # Extracts a part of the URL as folder name\n",
    "\n",
    "    # Create a folder for each base_url if it doesn't exist\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    for year in years:\n",
    "        # Format the URL with the current year\n",
    "        url = base_url.format(year=year)\n",
    "\n",
    "        # Define the full path for the output file\n",
    "        output_path = os.path.join(folder_name, f'pr_day_{folder_name}_historical_{year}.nc')\n",
    "\n",
    "        # Download the file and save it to the specified folder\n",
    "        try:\n",
    "            filename = wget.download(url, out=output_path)\n",
    "            print(f'\\nFile downloaded as {filename}')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}: {e}\")\n",
    "            continue  # Skip to the next file if download fails\n",
    "\n",
    "        # Process the downloaded file\n",
    "        try:\n",
    "            # Open the NetCDF file using xarray\n",
    "            dataset = xr.open_dataset(output_path)\n",
    "\n",
    "            # Extract the variable of interest (precipitation 'pr')\n",
    "            variable = dataset['pr']\n",
    "\n",
    "            # Extract data for the specific point using the nearest available grid point\n",
    "            point_data = variable.sel(lat=lat_brg, lon=lon_brg, method='nearest')\n",
    "\n",
    "            # Convert to a pandas DataFrame for easier manipulation and exporting\n",
    "            df = point_data.to_dataframe().reset_index()\n",
    "\n",
    "            # Define the output CSV file path\n",
    "            output_file = os.path.join(folder_name, f'pr_{folder_name}_{year}.txt')\n",
    "\n",
    "            # Save the data to a text file (tab-separated values)\n",
    "            df.to_csv(output_file, index=False, sep='\\t')\n",
    "\n",
    "            print(f'{filename} Precipitation Data for Year {year} extracted and saved to {output_file}')\n",
    "\n",
    "            # Close the dataset to free up resources\n",
    "            dataset.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {output_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a8f223-f78f-45f0-9b22-1242b12ccdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c6a0fc-e857-427b-b0b2-723a9148cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "import xarray as xr\n",
    "import cftime\n",
    "from xclim import sdba\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "file_path = 'Daily Data.csv'  # Replace with the path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Convert the 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True)\n",
    "\n",
    "# Step 3: Set 'Date' as the index for easier time-series handling\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Step 4: Extract relevant data from the DataFrame\n",
    "historical_gcm = df['Historical GCM'].values\n",
    "future_gcm = df['Future GCM'].values\n",
    "historical_obs = df['Observation'].values\n",
    "dates = df.index\n",
    "\n",
    "# Step 5: Create xarray DataArrays for Quantile Mapping\n",
    "# Define time coordinates using the date index\n",
    "t_hist = xr.cftime_range(start=dates.min(), end=dates.max(), freq=\"D\", calendar=\"standard\")\n",
    "\n",
    "# Create DataArrays for each time series\n",
    "hist_gcm = xr.DataArray(historical_gcm, dims=(\"time\",), coords={\"time\": t_hist}, attrs={\"units\": \"mm/day\"})\n",
    "future_gcm = xr.DataArray(future_gcm, dims=(\"time\",), coords={\"time\": t_hist}, attrs={\"units\": \"mm/day\"})\n",
    "hist_obs = xr.DataArray(historical_obs, dims=(\"time\",), coords={\"time\": t_hist}, attrs={\"units\": \"mm/day\"})\n",
    "\n",
    "# Step 6: Bias correction using Quantile Mapping from xclim's sdba module\n",
    "# Train the quantile mapping model on the historical GCM and historical observations\n",
    "qm = sdba.EmpiricalQuantileMapping.train(hist_obs, hist_gcm, nquantiles=100000, group=\"time\")\n",
    "\n",
    "# Step 7: Apply the bias correction to both historical and future GCM data\n",
    "corrected_hist_gcm = qm.adjust(hist_gcm, interp=\"linear\", extrapolation=\"constant\")\n",
    "corrected_future_gcm = qm.adjust(future_gcm, interp=\"linear\", extrapolation=\"constant\")\n",
    "\n",
    "# Step 8: Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates, historical_obs, label=\"Historical Observation\", color='black')\n",
    "plt.plot(dates, historical_gcm, label=\"Original Historical GCM\", color='blue')\n",
    "plt.plot(dates, corrected_hist_gcm, label=\"Corrected Historical GCM\", color='green')\n",
    "plt.plot(dates, future_gcm, label=\"Original Future GCM\", color='orange')\n",
    "plt.plot(dates, corrected_future_gcm, label=\"Corrected Future GCM\", color='red')\n",
    "plt.legend()\n",
    "plt.title('Bias Correction Using Quantile Mapping')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 9: Save the corrected data to a new CSV file\n",
    "corrected_df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Corrected Historical GCM': corrected_hist_gcm.values,\n",
    "    'Corrected Future GCM': corrected_future_gcm.values\n",
    "})\n",
    "corrected_df.to_csv('Corrected_Daily_Ens-W.csv', index=False)\n",
    "\n",
    "print(\"Corrected data saved to 'corrected_gcm_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a4cc4-21c0-4e37-95fc-690f47ba8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef40ed-1dcd-424b-b412-ad59c1f75dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate statistical indicators\n",
    "def calculate_statistics(observed, simulated):\n",
    "    mae = mean_absolute_error(observed, simulated)\n",
    "    mse = mean_squared_error(observed, simulated)\n",
    "    rmse = np.sqrt(mse)\n",
    "    bias = np.mean(simulated - observed)\n",
    "    correlation, _ = pearsonr(observed, simulated)\n",
    "    \n",
    "    nse = 1 - (np.sum((simulated - observed) ** 2) / np.sum((observed - np.mean(observed)) ** 2))\n",
    "    \n",
    "    pbias = 100 * np.sum(simulated - observed) / np.sum(observed)\n",
    "    \n",
    "    return {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'Bias': bias,\n",
    "        'Correlation': correlation,\n",
    "        'NSE': nse,\n",
    "        'PBIAS': pbias\n",
    "    }\n",
    "\n",
    "# Function to calculate statistics for each model and save to CSV\n",
    "def calculate_statistics_for_models(file_path):\n",
    "    # Load the CSV file\n",
    "    data_df = pd.read_csv(file_path)\n",
    "    \n",
    "\n",
    "    # Define the list of models (columns) excluding 'Date', 'Year', and 'Observation'\n",
    "    models = [col for col in data_df.columns if col not in ['Observation']]\n",
    "\n",
    "    # Observed data\n",
    "    observed = data_df['Observation']\n",
    "\n",
    "    # Initialize a dictionary to store results\n",
    "    results = {}\n",
    "\n",
    "    for model in models:\n",
    "        # Get simulated data for the current model\n",
    "        simulated = data_df[model].dropna()  # Remove NaN for statistical comparison\n",
    "        valid_observed = observed[:len(simulated)]  # Match the length of observed with simulated\n",
    "        \n",
    "\n",
    "        # Calculate statistical indicators\n",
    "        stats = calculate_statistics(valid_observed, simulated)\n",
    "\n",
    "        # Store the results in the dictionary with the model name as the key\n",
    "        results[model] = stats\n",
    "\n",
    "    # Convert results dictionary to DataFrame and transpose it\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save the transposed results to a CSV file\n",
    "    output_file = 'statistical_indices_M.csv'\n",
    "    results_df.to_csv(output_file, index=True)\n",
    "    \n",
    "    print(f\"Statistical indicators saved to {output_file}\")\n",
    "\n",
    "# Example file path (replace with your file path)\n",
    "file_path = 'MYDC+O.csv'\n",
    "\n",
    "# Call the function to calculate and save the statistics\n",
    "calculate_statistics_for_models(file_path)\n",
    "\n",
    "# Function to calculate climate indicators for a given precipitation series\n",
    "def calculate_climate_indicators(precipitation_series):\n",
    "    indicators = {}\n",
    "    \n",
    "    q99 = precipitation_series.quantile(0.99)\n",
    "#     print(q99)\n",
    "    # Annual Maximum Precipitation (AMP)\n",
    "    indicators['AMP'] = precipitation_series.max()\n",
    "    \n",
    "    # Mean Annual Precipitation (MAP)\n",
    "    indicators['MAP'] = precipitation_series.mean()\n",
    "    \n",
    "    # Frequency of Precipitation (P-FREQ): Days with precipitation > 1 mm\n",
    "    indicators['P-FREQ'] = (precipitation_series > q99).sum()\n",
    "    \n",
    "    # Heavy-to-Non-Heavy Precipitation Ratio (H-NH): Heavy days > 10 mm\n",
    "    heavy_days = (precipitation_series > q99).sum()\n",
    "    non_heavy_days = ((precipitation_series > 1) & (precipitation_series < q99)).sum()\n",
    "    indicators['H-NH'] = heavy_days / non_heavy_days if non_heavy_days != 0 else np.nan\n",
    "    \n",
    "    # Consecutive Dry Days (CDD): Max consecutive days with no precipitation (precipitation == 0)\n",
    "    is_dry = (precipitation_series < 0.2).astype(int)\n",
    "    cdd = (is_dry * (is_dry.groupby((is_dry != is_dry.shift()).cumsum()).transform('size'))).max()\n",
    "    indicators['CDD'] = cdd\n",
    "    \n",
    "    # Consecutive Wet Days (CWD): Max consecutive days with precipitation > 1 mm\n",
    "    is_wet = (precipitation_series > 1).astype(int)\n",
    "    cwd = (is_wet * (is_wet.groupby((is_wet != is_wet.shift()).cumsum()).transform('size'))).max()\n",
    "    indicators['CWD'] = cwd\n",
    "    \n",
    "    # Precipitation Intensity (P-INT): Mean precipitation on wet days (precipitation > 1 mm)\n",
    "    wet_days_precipitation = precipitation_series[precipitation_series > 1]\n",
    "    indicators['P-INT'] = wet_days_precipitation.mean() if len(wet_days_precipitation) > 0 else np.nan\n",
    "    \n",
    "#     # Fraction of Total Precipitation from Heavy Events (P-HEAVY): Precipitation > 10 mm\n",
    "#     total_precipitation = precipitation_series.sum()\n",
    "#     heavy_precipitation = precipitation_series[precipitation_series > 10].sum()\n",
    "#     indicators['P-HEAVY'] = heavy_precipitation / total_precipitation if total_precipitation != 0 else np.nan\n",
    "    \n",
    "#     # Annual Wet Days (P-WET): Number of days with precipitation > 1 mm\n",
    "#     indicators['P-WET'] = (precipitation_series > 1).sum()\n",
    "    \n",
    "#     # Annual Total Precipitation (P-TOT)\n",
    "#     indicators['P-TOT'] = precipitation_series.sum()\n",
    "    \n",
    "    return indicators\n",
    "\n",
    "# Step 1: Load the CSV file with Date and multiple GCM models\n",
    "file_path = 'Fill-HCD.csv'  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Ensure the 'Date' column is in datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Step 3: Set 'Date' as the index for easier time-based operations\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Step 4: Extract the GCM columns (excluding 'Date')\n",
    "gcm_columns = df.columns\n",
    "\n",
    "# Step 5: Create a dictionary to store the climate indicators for all GCM models\n",
    "climate_indicators_dict = {\n",
    "    'AMP': pd.DataFrame(),\n",
    "    'MAP': pd.DataFrame(),\n",
    "    'P-FREQ': pd.DataFrame(),\n",
    "    'H-NH': pd.DataFrame(),\n",
    "    'CDD': pd.DataFrame(),\n",
    "    'CWD': pd.DataFrame(),\n",
    "    'P-INT': pd.DataFrame(),\n",
    "#     'P-HEAVY': pd.DataFrame(),\n",
    "#     'P-WET': pd.DataFrame(),\n",
    "#     'P-TOT': pd.DataFrame()\n",
    "}\n",
    "\n",
    "# Step 6: Loop through each GCM model, calculate the indicators for each year\n",
    "for model in gcm_columns:\n",
    "    # Step 6.1: Group the data by year\n",
    "    df['Year'] = df.index.year\n",
    "    grouped = df.groupby('Year')\n",
    "    \n",
    "    # Step 6.2: Calculate the indicators for each year\n",
    "    for year, group in grouped:\n",
    "        precipitation_series = group[model]\n",
    "        indicators = calculate_climate_indicators(precipitation_series)\n",
    "#         print(model, year, indicators)\n",
    "        # Append the calculated indicators to the corresponding DataFrame for each climate indicator\n",
    "        for indicator_name, value in indicators.items():\n",
    "            climate_indicators_dict[indicator_name].loc[year, model] = value\n",
    "            \n",
    "# Step 7: Save the climate indicators to a new Excel file with each indicator as a separate sheet\n",
    "output_file = 'climate_indicators.xlsx'\n",
    "with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "    for indicator_name, indicator_df in climate_indicators_dict.items():\n",
    "        # Save each indicator DataFrame to a separate sheet\n",
    "        indicator_df.to_excel(writer, sheet_name=indicator_name)\n",
    "\n",
    "print(f\"Climate indicators saved to {output_file}\")\n",
    "# Function to shift the values and apply Min-Max scaling (to avoid negative values)\n",
    "def min_max_scaling(matrix):\n",
    "    shifted_matrix = np.zeros_like(matrix)  # Initialize an empty matrix for scaled values\n",
    "    for j in range(matrix.shape[1]):  # Loop through each indicator (column)\n",
    "        min_val = np.min(matrix[:, j])\n",
    "        max_val = np.max(matrix[:, j])\n",
    "        # Shift the values: Subtract the minimum, then divide by the (max - min) to get [0, 1] range\n",
    "        if max_val != min_val:  # Avoid division by zero\n",
    "            shifted_matrix[:, j] = (matrix[:, j] - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            shifted_matrix[:, j] = 1  # If max_val == min_val, set the entire column to 1\n",
    "    return shifted_matrix\n",
    "\n",
    "# Function to normalize the decision matrix after min-max scaling\n",
    "def normalize_matrix(matrix):\n",
    "    # Normalize each column (indicator) by dividing by the sum of each column\n",
    "    norm_matrix = matrix / matrix.sum(axis=0)\n",
    "    return norm_matrix\n",
    "\n",
    "# Function to calculate entropy-based weights using the four steps\n",
    "def entropy_weight_method(matrix):\n",
    "    # Step 1: Apply min-max scaling to shift the values to [0, 1] range\n",
    "    scaled_matrix = min_max_scaling(matrix)\n",
    "    \n",
    "    # Step 2: Normalize the matrix\n",
    "    norm_matrix = normalize_matrix(scaled_matrix)\n",
    "    \n",
    "    # Step 3: Calculate Entropy (E_nj)\n",
    "    T = norm_matrix.shape[0]  # Number of GCMs (alternatives)\n",
    "    entropy = np.zeros(norm_matrix.shape[1])  # Entropy for each indicator\n",
    "    for j in range(norm_matrix.shape[1]):  # Loop through indicators\n",
    "        for a in range(T):  # Loop through GCMs (alternatives)\n",
    "            if norm_matrix[a, j] > 0:  # Avoid log(0)\n",
    "                entropy[j] -= norm_matrix[a, j] * np.log(norm_matrix[a, j])\n",
    "        entropy[j] *= 1 / np.log(T)\n",
    "    \n",
    "    # Step 4: Calculate Degree of Diversification (D_dj)\n",
    "    diversification = 1 - entropy\n",
    "    \n",
    "    # Step 5: Calculate weights (r_j)\n",
    "    weights = diversification / diversification.sum()\n",
    "    \n",
    "    return weights, entropy, diversification\n",
    "\n",
    "# Read the decision matrix from a CSV file\n",
    "file_path = 'stat&clim-indices_M.csv'  # Update with your file path\n",
    "df = pd.read_csv(file_path, index_col=0)  # Assuming the first column contains indicator names\n",
    "\n",
    "# Convert the DataFrame to numeric values only\n",
    "df_numeric = df.apply(pd.to_numeric, errors='coerce')  # Convert all non-numeric to NaN\n",
    "\n",
    "# Drop any rows or columns that contain NaN values (optional, depending on your use case)\n",
    "df_clean = df_numeric.dropna()\n",
    "\n",
    "# Convert the DataFrame to a NumPy array (transpose for indicators in rows and alternatives in columns)\n",
    "matrix = df_clean.values.T\n",
    "\n",
    "# Calculate the weights and entropy using the entropy method\n",
    "weights, entropy, diversification = entropy_weight_method(matrix)\n",
    "\n",
    "# Output the results\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Entropy:\", entropy)\n",
    "print(\"Diversification:\", diversification)\n",
    "\n",
    "# Optionally save the results to a new CSV file\n",
    "results_df = pd.DataFrame({\n",
    "    'Indicator': df_clean.index,\n",
    "    'Weight': weights,\n",
    "    'Entropy': entropy,\n",
    "    'Diversification': diversification\n",
    "})\n",
    "results_df.to_csv('Weights-D.csv', index=False)\n",
    "\n",
    "print(\"Weights and entropy saved to Stat_entropy_weights.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file (assuming you've uploaded the file correctly)\n",
    "file_path = 'stat&clim-indices_MT.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assume that the first column is the alternatives (e.g., GCM models)\n",
    "# and the remaining columns are the indicators.\n",
    "alternatives = df.iloc[:, 0]  # Alternatives (GCM models)\n",
    "indicators = df.iloc[:, 1:]   # Indicator values\n",
    "\n",
    "# Add a name to the first column (e.g., GCM Model Name)\n",
    "alternatives.name = \"GCM Model Name\"\n",
    "\n",
    "# Define the weights (r_j) for each indicator (normalized weights)\n",
    "weights = np.array([0.1, 0.259, 0.142, 0.013, 0.019, 0.012, 0.013, 0.065, 0.054, 0.081, 0.091, 0.076, 0.077])  # Replace with actual weights\n",
    "\n",
    "# Normalize the indicator values across each column (indicator)\n",
    "normalized_indicators = (indicators - indicators.min()) / (indicators.max() - indicators.min())\n",
    "# print(normalized_indicators)\n",
    "# Calculate the ideal solution (K*) for each indicator (assumed as the max value here)\n",
    "# ideal_solution = normalized_indicators.min()\n",
    "ideal_solution =[0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \n",
    "# print(ideal_solution)\n",
    "# Define the value of p (1 for linear, 2 for Euclidean distance, etc.)\n",
    "p = 2\n",
    "\n",
    "# Function to calculate Lp norm for each alternative\n",
    "def calculate_Lp(norm_indicators, ideal_solution, weights, p):\n",
    "    Lp_distances = []\n",
    "    for _, row in norm_indicators.iterrows():\n",
    "        # Calculate the distance for each alternative\n",
    "        Lp = np.sum(weights * np.abs(ideal_solution - row)**p)**(1/p)\n",
    "        Lp_distances.append(Lp)\n",
    "    return np.array(Lp_distances)\n",
    "\n",
    "# Calculate Lp norm for each alternative\n",
    "Lp_distances = calculate_Lp(normalized_indicators, ideal_solution, weights, p)\n",
    "\n",
    "# Create a DataFrame to show the distances for each alternative\n",
    "results_df = pd.DataFrame({\n",
    "    'GCM Model': alternatives,\n",
    "    'Lp Distance': Lp_distances\n",
    "})\n",
    "\n",
    "# Rank the alternatives based on their Lp distance (smaller is better)\n",
    "results_df['Rank'] = results_df['Lp Distance'].rank(method='min')\n",
    "\n",
    "# Sort the models based on Lp distance (ascending order)\n",
    "sorted_results_df = results_df.sort_values(by='Lp Distance')\n",
    "print(sorted_results_df)\n",
    "\n",
    "# Write the sorted DataFrame to a CSV file\n",
    "output_file_path = 'Rank.csv'\n",
    "sorted_results_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
